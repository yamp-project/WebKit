/*
 * Copyright (c) 2021-2023 Apple Inc. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS'' AND ANY
 * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#import "config.h"
#import "PipelineLayout.h"

#import "APIConversions.h"
#import "BindGroupLayout.h"
#import "Device.h"
#import "ShaderStage.h"
#import <wtf/TZoneMallocInlines.h>

namespace WebGPU {

static void reportErrorInCreatePipelineLayout(NSString *error, bool isAutogenerated, Device& device)
{
    if (!isAutogenerated)
        device.generateAValidationError(error);
}

Ref<PipelineLayout> Device::createPipelineLayout(const WGPUPipelineLayoutDescriptor& descriptor, bool isAutogenerated)
{
    if (!isValid())
        return PipelineLayout::createInvalid(*this);

    std::optional<Vector<Ref<BindGroupLayout>>> optionalBindGroupLayouts = std::nullopt;
    if (auto descriptorBindGroupLayouts = descriptor.bindGroupLayoutsSpan(); !descriptorBindGroupLayouts.empty()) {
        auto& deviceLimits = limits();

        if (descriptorBindGroupLayouts.size() > deviceLimits.maxBindGroups) {
            reportErrorInCreatePipelineLayout([NSString stringWithFormat:@"Too many bind groups(%zu), limit(%u)", descriptorBindGroupLayouts.size(), deviceLimits.maxBindGroups], isAutogenerated, *this);
            return PipelineLayout::createInvalid(*this);
        }
        for (auto bindGroupLayout : descriptorBindGroupLayouts) {
            if (!bindGroupLayout)
                return PipelineLayout::createInvalid(*this);
        }
        Vector<Ref<BindGroupLayout>> bindGroupLayouts(descriptorBindGroupLayouts.size(), [&](size_t i) {
            return Ref<BindGroupLayout> { WebGPU::protectedFromAPI(descriptorBindGroupLayouts[i]) };
        });
        ShaderStage stages[] = { ShaderStage::Vertex, ShaderStage::Fragment, ShaderStage::Compute };
        for (ShaderStage shaderStage : stages) {
            uint32_t uniformBufferCount = 0, storageBufferCount = 0, samplerCount = 0, textureCount = 0, storageTextureCount = 0;
            for (auto& bindGroupLayout : bindGroupLayouts) {
                if (this != &bindGroupLayout->device()) {
                    reportErrorInCreatePipelineLayout(@"Device mismatch", isAutogenerated, *this);
                    return PipelineLayout::createInvalid(*this);
                }
                if (!bindGroupLayout->isValid()) {
                    reportErrorInCreatePipelineLayout(@"Bind group layout is invalid", isAutogenerated, *this);
                    return PipelineLayout::createInvalid(*this);
                }
                uniformBufferCount += bindGroupLayout->uniformBuffersPerStage(shaderStage);
                storageBufferCount += bindGroupLayout->storageBuffersPerStage(shaderStage);
                samplerCount += bindGroupLayout->samplersPerStage(shaderStage);
                textureCount += bindGroupLayout->texturesPerStage(shaderStage);
                storageTextureCount += bindGroupLayout->storageTexturesPerStage(shaderStage);
                if (uniformBufferCount > deviceLimits.maxUniformBuffersPerShaderStage || storageBufferCount > deviceLimits.maxStorageBuffersPerShaderStage || samplerCount > deviceLimits.maxSamplersPerShaderStage || textureCount > deviceLimits.maxSampledTexturesPerShaderStage || storageTextureCount > deviceLimits.maxStorageTexturesPerShaderStage) {
                    reportErrorInCreatePipelineLayout([NSString stringWithFormat:@"Resource usage limits exceeded: uniformBufferCount(%u) > deviceLimits.maxUniformBuffersPerShaderStage(%u) || storageBufferCount(%u) > deviceLimits.maxStorageBuffersPerShaderStage(%u) || samplerCount(%u) > deviceLimits.maxSamplersPerShaderStage(%u) || textureCount(%u) > deviceLimits.maxSampledTexturesPerShaderStage(%u) || storageTextureCount(%u) > deviceLimits.maxStorageTexturesPerShaderStage(%u)", uniformBufferCount, deviceLimits.maxUniformBuffersPerShaderStage, storageBufferCount, deviceLimits.maxStorageBuffersPerShaderStage, samplerCount, deviceLimits.maxSamplersPerShaderStage, textureCount, deviceLimits.maxSampledTexturesPerShaderStage, storageTextureCount, deviceLimits.maxStorageTexturesPerShaderStage], isAutogenerated, *this);
                    return PipelineLayout::createInvalid(*this);
                }
            }
        }

        uint32_t dynamicUniformBuffers = 0, dynamicStorageBuffers = 0;
        for (auto& bindGroupLayout : bindGroupLayouts) {
            dynamicUniformBuffers += bindGroupLayout->dynamicUniformBuffers();
            dynamicStorageBuffers += bindGroupLayout->dynamicStorageBuffers();
        }
        if (dynamicUniformBuffers > deviceLimits.maxDynamicUniformBuffersPerPipelineLayout) {
            reportErrorInCreatePipelineLayout([NSString stringWithFormat:@"Too many dynamic uniform buffers: used(%u), limit(%u)", dynamicUniformBuffers, deviceLimits.maxDynamicUniformBuffersPerPipelineLayout], isAutogenerated, *this);
            return PipelineLayout::createInvalid(*this);
        }
        if (dynamicStorageBuffers > deviceLimits.maxDynamicStorageBuffersPerPipelineLayout) {
            reportErrorInCreatePipelineLayout([NSString stringWithFormat:@"Too many dynamic storage buffers: used(%u), limit(%u)", dynamicStorageBuffers, deviceLimits.maxDynamicStorageBuffersPerPipelineLayout], isAutogenerated, *this);
            return PipelineLayout::createInvalid(*this);
        }

        optionalBindGroupLayouts = WTFMove(bindGroupLayouts);
    }

    if (m_pipelineLayoutId == std::numeric_limits<decltype(m_pipelineLayoutId)>::max()) {
        loseTheDevice(WGPUDeviceLostReason_Undefined);
        return PipelineLayout::createInvalid(*this);
    }
    return PipelineLayout::create(WTFMove(optionalBindGroupLayouts), isAutogenerated, ++m_pipelineLayoutId, *this);
}

bool PipelineLayout::isAutoLayout() const
{
    return m_isAutogenerated;
}

static void addInitialOffset(uint32_t initialOffset, uint32_t offset, uint32_t groupIndex, auto& offsets, auto& dynamicOffets)
{
    if (initialOffset != offset) {
        auto difference = offset - initialOffset;
        offsets.add(groupIndex, difference / sizeof(uint32_t));
        if (difference)
            dynamicOffets.add(groupIndex, initialOffset / sizeof(uint32_t));
    }
}

WTF_MAKE_TZONE_ALLOCATED_IMPL(PipelineLayout);

PipelineLayout::PipelineLayout(std::optional<Vector<Ref<BindGroupLayout>>>&& optionalBindGroupLayouts, bool isAutogenerated, uint64_t uniqueId, Device& device)
    : m_bindGroupLayouts(WTFMove(optionalBindGroupLayouts))
    , m_device(device)
    , m_uniqueId(uniqueId)
    , m_isAutogenerated(isAutogenerated)
{
    if (!m_bindGroupLayouts)
        return;

    if (m_isAutogenerated) {
        for (auto& bindGroupLayout : *m_bindGroupLayouts)
            bindGroupLayout->setAutogeneratedPipelineLayout(this);
    }

    auto& bindGroupLayouts = *m_bindGroupLayouts;
    auto bindGroupLayoutsSize = bindGroupLayouts.size();
    uint32_t vertexOffset = 0, fragmentOffset = 0, computeOffset = 0;
    for (size_t groupIndex = 0; groupIndex < bindGroupLayoutsSize; ++groupIndex) {
        auto& bindGroupLayout = bindGroupLayouts[groupIndex];
        uint32_t initialVertexOffset = vertexOffset, initialFragmentOffset = fragmentOffset, initialComputeOffset = computeOffset;
        vertexOffset += bindGroupLayout->sizeOfVertexDynamicOffsets();
        fragmentOffset += bindGroupLayout->sizeOfFragmentDynamicOffsets();
        computeOffset += bindGroupLayout->sizeOfComputeDynamicOffsets();

        addInitialOffset(initialVertexOffset, vertexOffset, groupIndex, m_vertexOffsets, m_vertexDynamicOffsets);
        addInitialOffset(initialFragmentOffset, fragmentOffset, groupIndex, m_fragmentOffsets, m_fragmentDynamicOffsets);
        addInitialOffset(initialComputeOffset, computeOffset, groupIndex, m_computeOffsets, m_computeDynamicOffsets);
        constexpr WGPUShaderStage stages[] = { WGPUShaderStage_Vertex, WGPUShaderStage_Fragment, WGPUShaderStage_Compute };
        for (auto stage : stages) {
            bool hasLinearLayout = true;
            std::optional<uint32_t> priorDynamicOffsetsIndex, firstDynamicOffsetsIndex;
            for (auto* entryPtr : bindGroupLayout->sortedEntries()) {
                auto& entry = *entryPtr;
                bool hasDynamicOffset = entry.vertexDynamicOffset || entry.fragmentDynamicOffset || entry.computeDynamicOffset;
                if (!hasDynamicOffset)
                    continue;

                if (entry.visibility & stage) {
                    auto dynamicOffsetsIndex = entry.dynamicOffsetsIndex;
                    if (priorDynamicOffsetsIndex && dynamicOffsetsIndex != *priorDynamicOffsetsIndex + 1)
                        hasLinearLayout = false;
                    priorDynamicOffsetsIndex = dynamicOffsetsIndex;
                    if (!firstDynamicOffsetsIndex)
                        firstDynamicOffsetsIndex = priorDynamicOffsetsIndex;
                }
            }
            if (hasLinearLayout && firstDynamicOffsetsIndex)
                m_linearLayoutOffset[shaderStage(stage)].set(groupIndex, std::make_pair(*firstDynamicOffsetsIndex, *priorDynamicOffsetsIndex - *firstDynamicOffsetsIndex + 1));
        }
    }
}

PipelineLayout::PipelineLayout(Device& device)
    : m_device(device)
    , m_isValid(false)
{
}

PipelineLayout::~PipelineLayout() = default;

void PipelineLayout::setLabel(String&&)
{
    // There is no Metal object that represents a pipeline layout.
}

bool PipelineLayout::operator==(const PipelineLayout& other) const
{
    UNUSED_PARAM(other);
    // FIXME: Implement this
    return false;
}

BindGroupLayout& PipelineLayout::bindGroupLayout(size_t i) const
{
    RELEASE_ASSERT(m_bindGroupLayouts.has_value());
    return (*m_bindGroupLayouts)[i];
}

BindGroupLayout* PipelineLayout::optionalBindGroupLayout(size_t i) const
{
    if (m_bindGroupLayouts.has_value() && i < m_bindGroupLayouts->size())
        return (*m_bindGroupLayouts)[i].ptr();

    return nullptr;
}

RefPtr<BindGroupLayout> PipelineLayout::protectedOptionalBindGroupLayout(size_t i) const
{
    return optionalBindGroupLayout(i);
}

void PipelineLayout::makeInvalid()
{
    m_isValid = false;
    if (m_bindGroupLayouts)
        m_bindGroupLayouts->clear();
}

static size_t returnTotalSize(auto& container)
{
    size_t totalSize = 0;
    for (auto& v : container) {
        ASSERT(v.value < std::numeric_limits<uint32_t>::max());
        totalSize += v.value;
    }

    return totalSize;
}

size_t PipelineLayout::sizeOfVertexDynamicOffsets() const
{
    return returnTotalSize(m_vertexOffsets);
}

size_t PipelineLayout::sizeOfFragmentDynamicOffsets() const
{
    return returnTotalSize(m_fragmentOffsets);
}

size_t PipelineLayout::sizeOfComputeDynamicOffsets() const
{
    return returnTotalSize(m_computeOffsets);
}

static std::optional<uint32_t> returnOffsetOfGroup0(auto& v, auto groupIndex)
{
    if (auto it = v.find(groupIndex); it != v.end())
        return it->value;

    return std::nullopt;
}

PipelineLayout::DynamicOffsetMapValue PipelineLayout::vertexOffsetForBindGroup(uint32_t groupIndex) const
{
    return returnOffsetOfGroup0(m_vertexDynamicOffsets, groupIndex);
}

PipelineLayout::DynamicOffsetMapValue PipelineLayout::fragmentOffsetForBindGroup(uint32_t groupIndex) const
{
    return returnOffsetOfGroup0(m_fragmentDynamicOffsets, groupIndex);
}

PipelineLayout::DynamicOffsetMapValue PipelineLayout::computeOffsetForBindGroup(uint32_t groupIndex) const
{
    return returnOffsetOfGroup0(m_computeDynamicOffsets, groupIndex);
}

bool PipelineLayout::offsetVectorForBindGroup(uint32_t bindGroupIndex, PipelineLayout::DynamicOffsetBufferMap& stageOffsets, const Vector<uint32_t>& dynamicOffsets, WGPUShaderStage stage, std::span<uint32_t> container)
{
    if (!m_bindGroupLayouts)
        return true;

    auto& linearLayoutOffset = m_linearLayoutOffset[shaderStage(stage)];
    if (auto it = linearLayoutOffset.find(bindGroupIndex); it != linearLayoutOffset.end()) {
        memcpySpan(container, dynamicOffsets.subspan(it->value.first, it->value.second));
        return true;
    }

    auto& bindGroupLayouts = *m_bindGroupLayouts;
    if (auto it = stageOffsets.find(bindGroupIndex); it != stageOffsets.end()) {
        uint32_t stageOffsetIndex = 0;
        if (bindGroupIndex >= bindGroupLayouts.size() || it->value > container.size())
            return false;
        auto& bindGroupLayout = bindGroupLayouts[bindGroupIndex];
        for (auto* entryPtr : bindGroupLayout->sortedEntries()) {
            auto& entry = *entryPtr;
            bool hasDynamicOffset = entry.vertexDynamicOffset || entry.fragmentDynamicOffset || entry.computeDynamicOffset;
            if (!hasDynamicOffset)
                continue;

            if (entry.visibility & stage) {
                auto dynamicOffsetsIndex = entry.dynamicOffsetsIndex;
                if (stageOffsetIndex >= container.size() || dynamicOffsetsIndex >= dynamicOffsets.size())
                    return false;
                container[stageOffsetIndex] = dynamicOffsets[dynamicOffsetsIndex];
                ++stageOffsetIndex;
            }
        }

        return true;
    }

    return true;
}

bool PipelineLayout::updateVertexOffsets(uint32_t bindGroupIndex, const Vector<uint32_t>& dynamicOffsets, std::span<uint32_t> destination)
{
    if (auto offset = vertexOffsetForBindGroup(bindGroupIndex))
        return offsetVectorForBindGroup(bindGroupIndex, m_vertexOffsets, dynamicOffsets, WGPUShaderStage_Vertex, destination.subspan(*offset));
    return true;
}

bool PipelineLayout::updateFragmentOffsets(uint32_t bindGroupIndex, const Vector<uint32_t>& dynamicOffsets, std::span<uint32_t> destination)
{
    if (auto offset = fragmentOffsetForBindGroup(bindGroupIndex))
        return offsetVectorForBindGroup(bindGroupIndex, m_fragmentOffsets, dynamicOffsets, WGPUShaderStage_Fragment, destination.subspan(*offset));
    return true;
}

bool PipelineLayout::updateComputeOffsets(uint32_t bindGroupIndex, const Vector<uint32_t>& dynamicOffsets, std::span<uint32_t> destination)
{
    if (auto offset = computeOffsetForBindGroup(bindGroupIndex))
        return offsetVectorForBindGroup(bindGroupIndex, m_computeOffsets, dynamicOffsets, WGPUShaderStage_Compute, destination.subspan(*offset));
    return true;
}

NSString* PipelineLayout::errorValidatingBindGroupCompatibility(const PipelineLayout::BindGroupHashMap& bindGroups) const
{
    if (!m_bindGroupLayouts)
        return nil;

    uint32_t setBindGroupsMaxValue = 0;
    for (auto it : bindGroups)
        setBindGroupsMaxValue = std::max(it.key, setBindGroupsMaxValue);

    auto& bindGroupLayouts = *m_bindGroupLayouts;
    auto numberOfBindGroupsInPipeline = bindGroupLayouts.size();
    if (setBindGroupsMaxValue + 1 < numberOfBindGroupsInPipeline) {
        if (numberOfBindGroupsInPipeline == 1 && !bindGroupLayouts[0]->entries().size())
            return nil;
        return [NSString stringWithFormat:@"number of bind groups set(%u) is less than the pipeline uses(%zu)", setBindGroupsMaxValue + 1, numberOfBindGroupsInPipeline];
    }

    for (size_t bindGroupIndex = 0; bindGroupIndex < numberOfBindGroupsInPipeline; ++bindGroupIndex) {
        if (!bindGroupLayouts[bindGroupIndex]->entries().size())
            continue;

        auto it = bindGroups.find(bindGroupIndex);
        if (it == bindGroups.end() || !it->value.get())
            return [NSString stringWithFormat:@"can not find bind group in pipeline for bindGroup index %zu", bindGroupIndex];

        RefPtr setBindGroupLayout = it->value->bindGroupLayout();
        if (!setBindGroupLayout)
            return [NSString stringWithFormat:@"can not find bind group in set bind groups for bindGroup index %zu", bindGroupIndex];

        auto& pipelineBindGroupLayout = bindGroupLayouts[bindGroupIndex];
        if (NSString* error = pipelineBindGroupLayout->errorValidatingBindGroupCompatibility(*setBindGroupLayout))
            return error;
    }

    return nil;
}

} // namespace WebGPU

#pragma mark WGPU Stubs

void wgpuPipelineLayoutReference(WGPUPipelineLayout pipelineLayout)
{
    WebGPU::fromAPI(pipelineLayout).ref();
}

void wgpuPipelineLayoutRelease(WGPUPipelineLayout pipelineLayout)
{
    WebGPU::fromAPI(pipelineLayout).deref();
}

void wgpuPipelineLayoutSetLabel(WGPUPipelineLayout pipelineLayout, const char* label)
{
    WebGPU::protectedFromAPI(pipelineLayout)->setLabel(WebGPU::fromAPI(label));
}
